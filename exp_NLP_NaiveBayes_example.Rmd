---
title: "NLP - Classification using a Naive Bayes classifier"
author: "Pier Lorenzo Paracchini"
date: "23 December 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = T, message = FALSE, warning = FALSE)
```

```{r requiredPackages}
require(caret)
require(tm)
require(wordcloud)
require(e1071)
require(MLmetrics)
```

* `caret` package
    * for splitting the data
* `tm` package, for NLP tasks
* `wordcloud` package, for visualizations
* `e1071` package, for the Naive Bayes Classifier implementation
* `MLmetrics` package, for a quick calculation of the evaluation coefficients
    * confusin matrix, accuracy, F1 score

## SMS messages: spam or ham?

_'Advertisers utilize Short Message Service (SMS) messages to target potential consumers with unwanted advertising. This kind of messages are known as SMS spam. Developing a classification algorithm that could filter SMS spam provides a useful tool for cellular phone providers.'_ ... and __Naive Bayes__ classifiers can be used to classify mobile phone sms messages as spam/ ham. 


## The Data

The data used for such playground activity is the [SMS Spam Collection v. 1](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/), a public set of SMS messages that have been collected for mobile phone spam research where each message has been properly labeled as __spam__ or __ham__.

The original file has been pre-processed in order to create a CSV file and it is available in the repository

* `\t` separator has been replaced by `,`
* `"` in the free text have been replaced by `'`
* the sms text has been included in `"` (quoted text)

Loading the data ...

```{r loadData}
rawData <-  read.csv("./data/smsspamcollection/SMSSpamCollection.txt", 
                     header = FALSE, 
                     stringsAsFactors = FALSE)
```

__Q__: What type of data is available in the dataset?

```{r dataOverview}
#Show the structure of the raw dataset
str(rawData)
```

The dataset includes `r dim(rawData)[1]` observations (sms) and `r dim(rawData)[2]` features/ columns. `r colnames(rawData)[1]` is the label, while   `r colnames(rawData)[2]` is the message text. Both features are stored as `character` vectors.

Some __possible improvements__ in the data, before doing any exploration, are

* to change the feature names to meaningfull names as 
    * `type` and 
    * `text` respectively.
* to __encode__ the `text` feature to `utf-8`, be sure about the encoding (potential __gremlings__ problems) 
* to __transform__ the `type` feature from a `character` type to a `factor` type...

```{r dataPreProcessing}
#Changing the name of the features/ columns
colnames(rawData) <- c("type", "text")

#Converting the text to utf-8 format
rawData$text <- iconv(rawData$text, to = "utf-8")

#Type as factor
rawData$type <- factor(rawData$type)

summary(rawData)
```

The data includes `r table(rawData$type)[1] + table(rawData$type)[2]` messages, `r table(rawData$type)[1]` ham messages and `r table(rawData$type)[2]` spam messages.

```{r dataPreProcessingInfo}
#Show the type of messages and their distributions
table(rawData$type)

#as percentage over the total messages
prop.table(table(rawData$type)) * 100
```

Please note how the data is __unbalanced__, there are lot of messages classifed as __ham__ and few messages as __spam__.

### Data Splitting

Before doing any exploration of the messages, the data is __split into a training and testing dataset__. The __training dataset__ is going to be __used for exploring the data and unserstanding the type of cleaning, transformation rules__ than need to be applied in order to create the relevant features to train the model/ classifier. 

The data splitting is done using a __stratified sampling__ approach, to keep the same proportions of the type pf messages within the training and testing datasets.

```{r dataSplitting}
set.seed(1234)
#Create a training set containing 75% of the data (with stratified sampling)
trainIndex <- createDataPartition(rawData$type, p = .75, 
                                  list = FALSE, 
                                  times = 1)
trainData <- rawData[trainIndex,]
testData <- rawData[-trainIndex,]

#proportion in train dataset
prop.table(table(trainData$type)) * 100

#proportion in test dataset
prop.table(table(testData$type)) * 100
```

# Exploratory Data Analysis

Exploratory data analysis is done on the training dataset only in order to have an hold out dataset, the testing dataset, that can be used to evaluate the trained model on a completely new set of data.  

__Q__: What is the content of the messages?

```{r explMessageExamples}
#Hame messages
trainData_ham <- trainData[trainData$type == "ham",]
head(trainData_ham$text)
tail(trainData_ham$text)

#spam messages
trainData_spam <- trainData[trainData$type == "spam",]
head(trainData_spam$text)
tail(trainData_spam$text)

trainData_spam <- NULL
trainData_ham <- NULL
```

Just looking at few samples it is possible to see some challenges

* contraction like `I'm`, `don't`
* abbreviations like `u`, `r`, `fr` 
* possible internet slang
* links & numbers - especially in the spam messages
    * this could be a possible hidden source of valuable information
        * how many numbers are in the message?
        * how many links are in the message?

SMS messages are strings of text composed of words, spaces, numbers, and punctuation. 

## Cleaning the data

When looking at the messages, it is needed to understand __what is the valuable information?__ and __what is noise?__. Strategies need to be defined on how to process and transform the data, e.g. how to

* break apart messages into sentences, and sentences into individual words
* process numbers, for examples
    * remove the numbers (simplest approach)
    * transform numbers into a KEYWORD e.g. aNUMBERa
* manage contractions like `I'm`, `don't`, etc.
* manage abbreviations quite common in social media messages
* manage punctuation, 
* handle uninteresting words such as and, but, etc. - such words are usually defined as stopwords.

All of those considerations are going to define the pipeline that is going to be used to clean/ transform the data before proceeding with feature engineering.

In this experiment a simplicistic approach is going to used for cleaning/ transforming the data 

* reduce all messages to lowe case
* remove numbers
* remove stopwords
* remove punctuations
* normalize whitespeces

__Note!__ The `tm` package - Text Mining package in R - is used for such purpose. Optionally the `tidytext` package could be used for working with tidy data.

```{r explCleaningData}
#create the corpus
corpus <- Corpus(VectorSource(trainData$text))
#basic info about the corpus
print(corpus)

#Inspect 4 documents
corpus[[1]]$content
corpus[[2]]$content
corpus[[50]]$content
corpus[[100]]$content

#1. normalize to lowercase (not a standard tm transformation)
corpus <- tm_map(corpus, content_transformer(tolower))
#2. remove numbers
corpus <- tm_map(corpus, removeNumbers)
#3. remove stopwords e.g. to, and, but, or (using predefined set of word in tm package)
corpus <- tm_map(corpus, removeWords, stopwords())
#4. remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#5. normalize whitespaces
corpus <- tm_map(corpus, stripWhitespace)

#Inspect the same 4 documents to visualize how the documents have been
#transformed
corpus[[1]]$content
corpus[[2]]$content
corpus[[50]]$content
corpus[[100]]$content
```

## Visual Analysis of the high frequency words (ham vs. spam)

Another interesting visualization involves comparing the clouds of SMS __spam__ and __ham__ ... 

```{r explWordClouds}
pal1 <- brewer.pal(9,"YlGn")
pal1 <- pal1[-(1:4)]

pal2 <- brewer.pal(9,"Reds")
pal2 <- pal2[-(1:4)]

#min.freq initial settings -> around 10% of the number of docs in the corpus (40 times)
par(mfrow = c(1,2))
wordcloud(corpus[trainData$type == "ham"], min.freq = 40, random.order = FALSE, colors = pal1)
wordcloud(corpus[trainData$type == "spam"], min.freq = 40, random.order = FALSE, colors = pal2)
```

Spam SMS messages include words such as urgent, free, mobile, call, claim, and stop; these terms do not appear in the ham cloud at all. Instead, ham messages use words such as can, sorry, need, and time. These stark differences suggest that our naive Bayes model will have some strong key words to differentiate between the classes.

# Feature Engineering

## Transforming the data: tokenization

Now that the messages are processed to our __liking__, the next step is to split the messages into individual elements through a process called __tokenization__. A __token__ is a single element of a text string; in this case, the tokens are words.

From the __corpus__ a data structured called __sparse matrix__ is created. In the __sparse matrix__, each row (observation) represents a document (SMS text message) and each column is a token/ word. The number in a cell represents the number of time the token (col) is present in the document represented by that row.

```{r featureEngTransformingData}
#Creation of the DTM considering terms with at least 2 chars
sms_dtm <- DocumentTermMatrix(corpus, control = list(global = c(2, Inf)))
#basic information about the sparse matrix
print(sms_dtm)
#To have an idea of the content of the document term matrix
inspect(sms_dtm[1:10, 5:13])
```

__Note!!__ The sparse matrix has the following dimensions `r dim(sms_dtm)[1]` documents and `r dim(sms_dtm)[2]` terms/ words where

* each row is a document/ message
* exch column is a term/ word/ feature

The sparse matrix needs to be transformed into a data structure that can be used to train a __naive Bayes classifier__. __Not all the terms/ words in the sparse matrix are useful for classification__. In order to reduce the number of features we can proceed to consider the words that appears at least a certain number of times (__frequent words__) and identify the features (terms dictionary).

```{r featEngTerms}
sms_features <- findFreqTerms(sms_dtm, 5) #find words that appears at least 5 times
summary(sms_features)
head(sms_features)
```

There are `r length(sms_features)` terms/ features idendified as frequent terms. __To limit our training and test matrix to only the words in the dictionary of frequent terms__ we can use the following commands ...

```{r featEngReduceSparseMatrixToFeatures, collapse=TRUE}
sms_dtm_train <- DocumentTermMatrix(corpus, list(global = c(2, Inf), dictionary = sms_features))
print(sms_dtm_train)
```

The naive Bayes classifier is typically trained on data with categorical features. This poses a problem since the cells in the sparse matrix indicate a count of the times a word appears in a message. We should change this to a factor variable that simply indicates yes or no depending on whether the word appears at all in a document.

```{r featEngNormalizeFeatures}
convert_counts <- function(x){
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x, levels = c(0,1), labels = c("No", "Yes"))
  return (x)
}
sms_dtm_train <- apply(sms_dtm_train, MARGIN = 2, convert_counts)
```

Now we have a matrix each with a "Yes" or "No" indicating if a specific word (feature) appears in the documents/ messages (rows).

```{r showCategoralizedSparseMatrix}
head(sms_dtm_train[,1:5])
```

# Train the model

The model can be train using the corpus created using the training dataset and the training classification labels

```{r trainTheModel}
sms_classifier <- naiveBayes(sms_dtm_train, trainData$type)

sms_classifier[[2]][1:5]
```

For each terms/ words available in `sms_features` - the features - probabilities are given. Such probabilities are used for calculating the Bayesian probabilities of a message being __ham__ or __spam__. 

# Evaluate the model

The evaluation of the model is performed using the testing dataset. The testing dataset needs to go through the same cleaning & feature engineering processes applied to the training dataset.

## Cleaning & Feature Engineering

```{r evaluatePrepareTestingDataset}
corpus <- Corpus(VectorSource(testData$text))
#1. normalize to lowercase (not a standard tm transformation)
corpus <- tm_map(corpus, content_transformer(tolower))
#2. remove numbers
corpus <- tm_map(corpus, removeNumbers)
#3. remove stopwords e.g. to, and, but, or (using predefined set of word in tm package)
corpus <- tm_map(corpus, removeWords, stopwords())
#4. remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#5. normalize whitespaces
corpus <- tm_map(corpus, stripWhitespace)

sms_dtm_test <- DocumentTermMatrix(corpus, list(global = c(2, Inf), dictionary = sms_features))
print(sms_dtm_test)

sms_dtm_test <- apply(sms_dtm_test, MARGIN = 2, convert_counts)
sms_dtm_test[1:10, 5:12]
```

## Evaluate the model

```{r evaluateTheModel, collapse=TRUE}
sms_test_pred <- predict(sms_classifier, sms_dtm_test)

#table actual (row) vs. predicted (col): confusion matrix
table(testData$type, sms_test_pred)

ConfusionMatrix(sms_test_pred, testData$type)

Accuracy(sms_test_pred, testData$type)

F1_Score(sms_test_pred, testData$type)
```

Looking at the table we can see that 30 messages out of 1392 messages (`r 30/ 1392`) have been incorrectly classified as spam or ham. The model has an accuracy of `r Accuracy(sms_test_pred, testData$type)` & an F1 score of `r F1_Score(sms_test_pred, testData$type)`. 

